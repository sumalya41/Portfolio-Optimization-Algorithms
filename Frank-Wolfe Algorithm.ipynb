{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "application_hw2 .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pYoSB7FUCO4J",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sympy.solvers import solve\n",
        "from sympy import Symbol\n",
        "\n",
        "filename = \"russell_prices.txt\"\n",
        "data = pd.read_csv(filename, sep=\" \", skiprows = [0,1], header=None)\n",
        "data = data.iloc[:,:-1]\n",
        "data_pct = data.T.pct_change().iloc[1:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vq2sNO4ICXAw",
        "colab": {}
      },
      "source": [
        "#1\n",
        "mean_return = data_pct.mean()\n",
        "cov_matrix = data_pct.cov()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5BWyNP-PCnJn",
        "colab": {}
      },
      "source": [
        "#2 Implement of algorithm\n",
        "\n",
        "def feasible_initial(l,u):    #Initialization of lower bounds and upper bounds of each asset in the portfolio and return a feasible portfolio vector\n",
        "\n",
        "    if l.sum()>1 or (~(l<=u)).sum()!=0:     \n",
        "        print(\"Infeasible!\")\n",
        "    elif (u.cumsum())[-1]<1:\n",
        "        print(\"infeasible!\")       \n",
        "    else:\n",
        "        w=l.copy()\n",
        "        count=0+l.sum()\n",
        "        for i in range(len(w)):\n",
        "            count+=u[i]-l[i] \n",
        "       \n",
        "            if count<1:\n",
        "              w[i]=u[i]\n",
        "            elif count==1:\n",
        "              w[i]=u[i]\n",
        "              break\n",
        "            else:\n",
        "              w[i]=u[i]-(count-1)\n",
        "              break\n",
        "        return w\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R2d25S2ICpGX",
        "colab": {}
      },
      "source": [
        "def Grad_Fx_wrt_x(lamda,w):\n",
        "    grad=2*lamda*np.dot(cov_matrix,w)-mean_return\n",
        "    return grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ezfAG1yTCpP7",
        "colab": {}
      },
      "source": [
        "def get_y(lamda,w, l, u):\n",
        "  # sort the asset indices so that the gradient is from large to small and get a sorted y vector\n",
        "  # loop m through 1 to 947\n",
        "  # get a list of feasible y and choose the y that gives the min grad*y\n",
        "  \n",
        "  feasible_y=[]\n",
        "  grad = Grad_Fx_wrt_x(lamda,w)\n",
        "  grad_sorted = grad.argsort()[::-1]\n",
        "  w_sorted = w[grad_sorted]\n",
        "  for m in range(947):\n",
        "    y=l-w_sorted\n",
        "    y[m]=0\n",
        "    y[m+1:]=(u-w_sorted)[m+1:]\n",
        "    y[m]=0-np.sum(y)\n",
        "    if y[m]>=(l-w_sorted)[m] and y[m]<=(u-w_sorted)[m]:    #use y[m] to check feasibility\n",
        "      feasible_y.append(list(y))\n",
        "  #print(\"len(feasible_y):\",len(feasible_y))               #there will only be 1 feasible candidates    \n",
        "\n",
        "  if len(feasible_y)==0:                    # when all candidate vectors are rejected, return an array of 0\n",
        "    return np.repeat(0,len(w))\n",
        "  \n",
        "  min_grad_y=0\n",
        "  y_opt=[]\n",
        "  for i in feasible_y:\n",
        "    if np.dot(grad[grad_sorted],i)<min_grad_y:           # if more than one candidate vector y give a same objective       \n",
        "        min_grad_y=np.dot(grad[grad_sorted],i)           # we choose the y that comes first\n",
        "        y_opt=i\n",
        "  if y_opt == []:\n",
        "    return np.repeat(0,len(w))\n",
        "  \n",
        "  # make the indices of the optimal y vector back to the original sequence\n",
        "  y_opto = list(np.repeat(0,947))\n",
        "  count = 0\n",
        "  for i in grad_sorted: \n",
        "    y_opto[i] = y_opt[count]\n",
        "    count = count+1\n",
        "    if count == len(grad_sorted-1):\n",
        "      break\n",
        "    \n",
        "  return np.array(y_opto)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "45iunUKMyqIg",
        "colab": {}
      },
      "source": [
        "def step_size(lamda,w,y):\n",
        "  # derivative of F(x+sy) wrt.s = y*F(x+sy)=0\n",
        "    s_up=np.dot(mean_return,y)/(2*lamda)-np.dot(np.dot(cov_matrix,w),y)\n",
        "    s_down=np.dot(np.dot(cov_matrix,y),y)\n",
        "    s=(s_up/s_down)\n",
        "    if s<0:\n",
        "        s=0\n",
        "    elif s>1:\n",
        "        s=1\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1QoAgyJt-Op",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def improve_f(lamda):\n",
        "  # loop to get optimal F(x)\n",
        "\n",
        "    l=np.repeat(0.0,947)      #have to enter an array of float \n",
        "    u=np.repeat(1,947)\n",
        "    w = feasible_initial(l,u)\n",
        "    \n",
        "    #Get current portfolio F\n",
        "    var_port,ret_port = var_ret(w)\n",
        "    var_port = float(var_port.iloc[0][0])\n",
        "    ret_port = float(ret_port.iloc[0])\n",
        "\n",
        "    F_old = lamda * var_port - ret_port\n",
        "    \n",
        "    #Proceed to the next position w, and get new F value\n",
        "    y=get_y(lamda,w, l, u)\n",
        "    s=step_size(lamda, w,y)\n",
        "    w=w+s*y\n",
        "    \n",
        "    var_port,ret_port = var_ret(w)\n",
        "    var_port = float(var_port.iloc[0][0])\n",
        "    ret_port = float(ret_port.iloc[0])\n",
        "    F_new = lamda * var_port - ret_port\n",
        "    \n",
        "    #Calculate the improvement of the step\n",
        "    dif = F_old - F_new\n",
        "    \n",
        "    #Keep finding new step until improvement is less than certain number\n",
        "    while dif > 0.0000001:\n",
        "      F_old = F_new\n",
        "      y=get_y(lamda,w, l, u)\n",
        "      s=step_size(lamda, w,y)\n",
        "      w=w+s*y\n",
        "      \n",
        "      var_port,ret_port = var_ret(w)\n",
        "      var_port = float(var_port.iloc[0][0])\n",
        "      ret_port = float(ret_port.iloc[0])\n",
        "\n",
        "      F_new = lamda * var_port - ret_port\n",
        "      dif = F_old - F_new\n",
        "   \n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F8WbQkJ5njKw",
        "colab": {}
      },
      "source": [
        "def var_ret(w):                  #This function takes a portfolio as input and return the portfolio's return and variance\n",
        "    w_vector = pd.DataFrame(w)\n",
        "    var = w_vector.transpose().dot(cov_matrix).dot(w_vector)\n",
        "    ret = w_vector.transpose().dot(mean_return)\n",
        "    return var, ret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X9Nhad2Keq2I",
        "outputId": "b0ca844c-890e-4afa-8f84-833a8275a9b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "#3 When lambda = 0\n",
        "lamda = 0\n",
        "l = np.repeat(0.0, 947)\n",
        "u = np.repeat(1, 947)\n",
        "w = feasible_initial(l,u)\n",
        "y = get_y(lamda, w, l, u)\n",
        "s = step_size(lamda, w, y)\n",
        "w = w + s*y\n",
        "var_0= var_ret(w)[0]\n",
        "\n",
        "print('Number of asset in the basket:',np.sum(w))\n",
        "print('When lambda is 0, we do not consider risk in the portfolio, thus the basket has 1 asset of the highest return.')\n",
        "print('variance of portfolio when lambda = 0 is:', var_0.iloc[0,0])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of asset in the basket: 1.0\n",
            "When lambda is 0, we do not consider risk in the portfolio, thus the basket has 1 asset of the highest return.\n",
            "variance of portfolio when lambda = 0 is: 0.018636783933845803\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XBXSxOtZmjTo",
        "outputId": "7a70d703-62fc-40b5-8f58-1b123a396714",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "#4 When lambda takes range from 1 to 5, step size 0.1\n",
        "\n",
        "var_array = np.array([])\n",
        "ret_array = np.array([])\n",
        "lamda_array = np.array([])\n",
        "\n",
        "for i in np.arange(1,5,0.1):\n",
        "  \n",
        "  w=improve_f(i)\n",
        "  var, ret = var_ret(w)\n",
        "  var_array = np.append(var_array,var)\n",
        "  ret_array = np.append(ret_array,ret)\n",
        "  lamda_array = np.append(lamda_array, i)\n",
        "  \n",
        "  \n",
        "plt.scatter(var_array, ret_array)\n",
        "plt.xlabel('variance')\n",
        "plt.ylabel('return')\n",
        "plt.xlim(0, 0.0008)\n",
        "plt.ylim(0, 0.005)\n",
        "plt.show\n",
        "print('The smallest variance on the graph is ', var_array.min(), ', it happens when lambda is ', lamda_array[var_array.argmin()])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The smallest variance on the graph is  0.00013378265231662605 , it happens when lambda is  4.900000000000004\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ8AAAEKCAYAAADNSVhkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwP1MIAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E4261aIvs0yl",
        "outputId": "4af007e2-75cb-42ae-e392-2db8eb6329bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#5\n",
        "S_0 = 0.018636783933845803\n",
        "S_min = var_array.min()\n",
        "S_mid = 0.5*(S_0+S_min)\n",
        "print('S_mid: ',S_mid)\n",
        "\n",
        "lamda = 0.0946                 #estimation of lambda to achieve this variance\n",
        "w=improve_f(lamda)\n",
        "\n",
        "var = var_ret(w)[0].iloc[0,0]\n",
        "print('At lambda = 0.0946, variance of the portfolio is: ', var)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "S_mid:  0.009385283293081215\n",
            "At lambda = 0.0946, variance of the portfolio is:  0.00938488669375063\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TScL0ZRl-zkf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}